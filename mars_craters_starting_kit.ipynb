{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Paris Saclay Center for Data Science](http://www.datascience-paris-saclay.fr)\n",
    "\n",
    "## [RAMP](https://www.ramp.studio/problems/mars_craters) on Mars craters detection and classification\n",
    "\n",
    "_Frédéric Schmidt (GEOPS), Anthony Lagain (GEOPS), Joris van den Bossche(CDS), Alexandre Boucaud (CDS)_\n",
    "    \n",
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "\n",
    "\n",
    "## The prediction task\n",
    "\n",
    "The goal of this RAMP is to classify correctly handwritten digits. For each submission, you will have to provide an image preprocessor (to standardize, resize, crop, augment images) and batch classifier, which will fit a training set and predict the classes (species) on a test set. The images are big so loading them into the memory at once is impossible. The batch classifier therefore will access them through a generator which can be \"asked for\" a certain number of training and validation images at a time. You will typically run one minibatch of stochastique gradient descent on these images to train a deep convolutional neural networks which are the state of the art in image classification.\n",
    "\n",
    "## Hints\n",
    "\n",
    "First of all, even though 68K images is relatively small compared to industrial level data sets, to achieve state-of-the-art performance, you will need big networks which will take ages (days) to train on a CPU. If you want to have a faster turnaround for tuning your net, you will need a GPU-equipped server of could instance. Setting up an AWS instance is easy, just follow [this tutorial](https://medium.com/@mateuszsieniawski/keras-with-gpu-on-amazon-ec2-a-step-by-step-instruction-4f90364e49ac#.dariq7i2u). If you want to have the starting kit preinstalled, use the community AMI \"pollenating_insects_2_users\".\n",
    "\n",
    "Your main bottleneck is memory. E.g., increasing the resolution to 128x128, you will need to decrease batch size. You should always run user_test_submission.py on the AWS node before submitting.\n",
    "\n",
    "For learning the nuts and bolts of convolutional nets, we suggest that you follow [Andrej Karpathy’s excellent course](http://cs231n.github.io).\n",
    "\n",
    "You have some trivial \"classical\" options to explore. You should set the epoch size to something more than three (in the starting kit). You should check when the validation error curve flattens because you will also be graded on training and test time. You can change the network architecture, apply different regularization techniques to control overfitting, optimization options to control underfitting.\n",
    "\n",
    "You can use pretrained nets from [here](https://github.com/fchollet/deep-learning-models). There are a couple of examples in the starting kit. Your options are the following.\n",
    "\n",
    "* Retrain or not the weights. If you do not, you are using the pretrained net as fixed a feature extractor. You can add some layers on the top of the output of the pretrained net, and only train your layers. If you retrain all the layers, you use the pretrained net as an initialization. Again, your goal is not only to increase accuracy but also to be frugal. Retraining the full net is obviously more expensive.\n",
    "* You can \"read out\" the activations from any layer, you do not need to keep the full net, not even the full convolutional stack.\n",
    "* The starting kit contains examples with the VGG16 net, but feel free to use any of the other popular nets. Just note that there is no way to change the architecture of these nets. In particular, each net expects images of a given dimension so your image preprocessing needs to resize or crop the images to the right size.\n",
    "\n",
    "\n",
    "You can also adjust the image preprocessing. Resizing to small (64x64 or even 32x32) will make the training faster so you can explore more hyperparameters, but the details will be lost so your final result will probably be suboptimal. Insects are mostly centered in the images but there are a lot of smaller insects which could be cropped for a better performance. You can also rotate the images or apply other data augmentation tricks (google \"convolutional nets data augmentation\"). You should also look at the actual images to get some inspiration to find meaningful preprocessing ideas.\n",
    "\n",
    "You can also get inspired by looking at the submissions of the [first edition](http://www.ramp.studio/events/pollenating_insects_M1XMAP583_M2HECXMAP542_201617) (after signing up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib import cm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the images are not yet in 'imgs', change the type of the cell to \"Code\" and run it. Will take ~1h, depending on connection speed. If you are using the AWS AMI \"pollinating_insects_2_users\", the images are already pre-installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "X_df = df['id']\n",
    "y_df = df['class']\n",
    "X = X_df.values\n",
    "y = y_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id\n",
       "class      \n",
       "0      4742\n",
       "1      5370\n",
       "2      4795\n",
       "3      4914\n",
       "4      4675\n",
       "5      4337\n",
       "6      4709\n",
       "7      4998\n",
       "8      4707\n",
       "9      4753"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_counts_df = df.groupby('class').count()\n",
    "labels_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worthwhile to look at some image panels, grouped by label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEWCAYAAAC9hIj8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYROMYRjesAnGMAxv/D9kZ8ZrkMmxuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113fe4190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_rows = 4\n",
    "nb_cols = 4\n",
    "nb_elements = nb_rows * nb_cols\n",
    "label = 8\n",
    "\n",
    "df_given_label = df[df['class']==label]\n",
    "\n",
    "subsample = np.random.choice(df_given_label['id'], replace=False, size=nb_elements)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "grid = AxesGrid(fig, 111, # similar to subplot(141)\n",
    "                nrows_ncols = (nb_rows, nb_cols),\n",
    "                axes_pad = 0.05,\n",
    "                label_mode = \"1\",\n",
    ")\n",
    "for i, image_id in enumerate(subsample):\n",
    "    filename = 'data/imgs/{}.png'.format(image_id)\n",
    "    image = imread(filename)\n",
    "    im = grid[i].imshow(image, cmap='Greys', interpolation='nearest')\n",
    "    grid[i].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images have size 28 $\\times$ 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_subsample = 1000\n",
    "shapes = np.empty((n_subsample, 2))\n",
    "for i, image_id in enumerate(X_df[:n_subsample]):\n",
    "    filename = 'data/imgs/{}.png'.format(image_id)\n",
    "    image = imread(filename)\n",
    "    shapes[i] = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28.0</th>\n",
       "      <th>28.0</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "height width       \n",
       "28.0   28.0    1000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes_df = pd.DataFrame(shapes, columns=['height', 'width'])\n",
    "shapes_df['count'] = 0\n",
    "shapes_df.groupby(['height', 'width']).count().sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x115c1cdd0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEACAYAAABcXmojAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1178d1d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shapes_df['height'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image preprocessing\n",
    "\n",
    "In the first workflow element image_preprocessor.py you can resize, crop, or rotate the images. This is an important step. Neural nets need standard-size images defined by the dimension of the input layer. MNIST images are centered and resized, so these operations are unlikely to be useful but rotation may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116a1c990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = 'data/imgs/{}.png'.format(X_df[161])\n",
    "image = imread(filename)\n",
    "plt.imshow(image, cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we resize the images to different resolutions, then blow them up so the difference can be visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAACYCAYAAAA/SfbgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA4lJREFUeJzt3UFK61AYhuEbLVSnTgRnDrsDpYh0MeJABN2DO3AFDsV1\nuAZ1ASrOnYnWqXjJKW3zJaZ9nlHxJ+Rg5fVA0rSaTqf/ABI2ul4AsLoEBogRGCBGYIAYgQFiBAaI\nERggRmCAGIEBYgQGiBl0eG6fUVh91Y/X3u/VV/3+gR0MECMwQIzAADECA8QIDBAjMECMwAAxAgPE\nCAwQIzBAjMAAMQIDxAgMENPlp6l75/HxsTh/e3urnR0fHze9HMJGo9HCxz48PDS4kv6ygwFiBAaI\nERggRmCAGIEBYgQGiBEYIMZ9MHPY3d0tzieTSe3s9fW16eXAn2cHA8QIDBAjMECMwAAxAgPECAwQ\n4zL1HLa3t4vz9/f32tnLy0vx2L29vYXWRM7JyUnXS+g9OxggRmCAGIEBYgQGiBEYIEZggBiBAWLc\nBzOHra2t4nw8HtfObm9vi8deXl4utCb4y+xggBiBAWIEBogRGCBGYIAYgQFiXKZu0P7+fu3s+fm5\nxZXQhLOzs66X0Ht2MECMwAAxAgPECAwQIzBAjMAAMQIDxLgPpkGDgV8n/GQHA8QIDBAjMECMwAAx\nAgPECAwQIzBAjBs3GnRwcFA7u7u7a3ElNGE4HC587PX19VLnPj8/X+r4v8IOBogRGCBGYIAYgQFi\nBAaIERggxmXqBh0eHtbOTk9Pi8d+fX0V5xsb/hfQP/5qgRiBAWIEBogRGCBGYIAYgQFiBAaIqabT\naVfn7uzEKZ+fn7WzWV9p8vHxUZz39CtRqh+vV+79LhmPx0sdf39/39BKWlX9/oEdDBAjMECMwAAx\nAgPECAwQIzBATC+vffbR5uZm10uA1tnBADECA8QIDBAjMECMwAAxAgPECAwQ43ENLbm4uCjOJ5NJ\ncX50dFQ729nZWWhNLVjbxzVU1X9PLphL6dEfs3T4FTce1wC0R2CAGIEBYgQGiBEYIEZggBiXqVvy\n9PRUnF9dXRXnNzc3tbMOL0vO4jL1glymBphBYIAYgQFiBAaIERggRmCAGIEBYtwHQ9La3gezptwH\nA7RHYIAYgQFiBAaIERggRmCAGIEBYgQGiBEYIEZggBiBAWIEBogRGCBGYICYQYfnXu57Hegb7/ca\nsoMBYgQGiBEYIEZggBiBAWIEBogRGCBGYIAYgQFiBAaIERggRmCAGIEBYgQGiBEYIEZggBiBAWIE\nBogRGCBGYICYb9dCWTWWOYG9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116cd5790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "nb_rows = 1\n",
    "nb_cols = 2\n",
    "nb_elements = nb_rows * nb_cols\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "grid = AxesGrid(fig, 111, # similar to subplot(141)\n",
    "                nrows_ncols = (nb_rows, nb_cols),\n",
    "                axes_pad = 0.05,\n",
    "                label_mode = \"1\",\n",
    ")\n",
    "grid[0].imshow(\n",
    "    resize(resize(image, (16, 16)), (224, 224), order=0),\n",
    "    cmap='Greys', interpolation='nearest')\n",
    "grid[0].axis('off')\n",
    "grid[1].imshow(\n",
    "    resize(resize(image, (8, 8)), (224, 224), order=0),\n",
    "    cmap='Greys', interpolation='nearest')\n",
    "grid[1].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we rotate the image. Explore options in skimage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAADGCAYAAADPPnYvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117b266d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.transform import rotate\n",
    "\n",
    "nb_rows = 1\n",
    "nb_cols = 4\n",
    "nb_elements = nb_rows * nb_cols\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "grid = AxesGrid(fig, 111, # similar to subplot(141)\n",
    "                nrows_ncols = (nb_rows, nb_cols),\n",
    "                axes_pad = 0.05,\n",
    "                label_mode = \"1\",\n",
    ")\n",
    "grid[0].imshow(rotate(image, 30), cmap='Greys', interpolation='nearest')\n",
    "grid[0].axis('off')\n",
    "grid[1].imshow(rotate(image, 45), cmap='Greys', interpolation='nearest')\n",
    "grid[1].axis('off')\n",
    "grid[2].imshow(rotate(image, 60), cmap='Greys', interpolation='nearest')\n",
    "grid[2].axis('off')\n",
    "grid[3].imshow(rotate(image, 75), cmap='Greys', interpolation='nearest')\n",
    "grid[3].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these tansformations should be implemented in the transform function found in the `image_preprocessor` workflow element that you will submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline\n",
    "\n",
    "For submitting at the [RAMP site](http://ramp.studio), you will have to write two classes, saved in two different files:   \n",
    "* the function `image_preprocessor`, which will be used to transform the images. \n",
    "* a class `BatchClassifier` that will handle the training and prediction using sample generators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessor\n",
    "\n",
    "MNIST contains well-centered and aligned images so we do nothing in the image preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submissions/starting_kit/image_preprocessor.py\n"
     ]
    }
   ],
   "source": [
    "%%file submissions/starting_kit/image_preprocessor.py\n",
    "def transform(x):\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The batch classifier\n",
    "\n",
    "Your convnet models will be implemented in the `batch_classifier.py` workflow element. The model is usually implemented in a separate build_model function which is called from init. It does not do any computation, just building the network architecture. Your main task is to find the right architecture for this data set. Every parameter is \"free\": the size of the input layer (which should match the preprocessing you do in `image_preprocessing.transform`), the number and composition of the convolutional blocks, the size and number of filters in each layer, the number and size of the fully connected layers, the regularization that you apply (usually only on the fully connected layers), and the optimizer. In addition, in the fit function you shold decide the size of the minibatches and the number of epochs. \n",
    "\n",
    "The starting kit is using [keras](https://keras.io). MNIST could be loaded into memory at once, but in general, image data sets are too big for this, so instead of a set of images, your `batch_classifier.fit` function will receive a <i>generator</i>. The type of this generator is `BatchGeneratorBuilder` that you can find in [`rampwf.workflows.image_classifier`](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/rampwf/workflows/image_classifier.py). You will interact with it by calling its `get_train_valid_generators` function. This function will still not return data, since typically you would call the `fit` function of a keras neural net with it, which would again result in filling the memory too fast. Instead, it returns two other generators that you will pass to the `fit_generator` function of a keras model. Why two? The first generator will generate _training_ images for keras, and the second generator generates _validation_ images, which you can use to monitor the learning curves when developing the model. Normally you will not need validation samples in your submission (only when you develop your models outside of the RAMP server) unless you implement automatic early stopping based on validation accuracy.\n",
    "\n",
    "The `BatchGeneratorBuilder.get_train_valid_generators` function expects two parameters:\n",
    "* `valid_ratio`: the ratio of the minibatch that should be used for validation, typically 0.1, and\n",
    "* `batch_size`: the number of images keras' `fit_generator` will get for every minibatch.\n",
    "The batch size is an important hyperparameter. Statistically, a larger batch size means more precise gradient estimates but computationally slower steps. More importantly, you will have to carefully set the batch size for staying within the memory, depending on the input image size, your network architecture, and your backend. In the starting kit we use 32 images per minibatch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting submissions/starting_kit/batch_classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%file submissions/starting_kit/batch_classifier.py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from rampwf.workflows.image_classifier import get_nb_minibatches\n",
    "\n",
    "\n",
    "class BatchClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def fit(self, gen_builder):\n",
    "        batch_size = 32\n",
    "        gen_train, gen_valid, nb_train, nb_valid =\\\n",
    "            gen_builder.get_train_valid_generators(\n",
    "                batch_size=batch_size, valid_ratio=0.1)\n",
    "        self.model.fit_generator(\n",
    "            gen_train,\n",
    "            # Total number of steps (batches of samples) to yield from\n",
    "            # generator before declaring one epoch finished and starting the\n",
    "            # next epoch. It should typically be equal to the number of unique\n",
    "            # samples of your dataset divided by the batch size.\n",
    "            steps_per_epoch=get_nb_minibatches(nb_train, batch_size),\n",
    "            epochs=1,\n",
    "            # In parallel to training, a CPU process loads and preprocesses\n",
    "            # data from disk and put it into a queue in the form of\n",
    "            # mini-batches of size `batch_size`.`max_queue_size` controls the\n",
    "            # maximum size of that queue. The size of the queue should be big\n",
    "            # enough so that the training process (GPU) never\n",
    "            # waits for data (the queue should be never be empty).\n",
    "            # The CPU process loads chunks of 1024 images each time, and\n",
    "            # 1024/batch_size mini-batches from that chunk are put into the\n",
    "            # queue. Assuming training the model on those 1024/batch_size\n",
    "            # mini-batches is slower than loading a single chunk of 1024\n",
    "            # images, a good lower bound for `max_queue_size` would be\n",
    "            # (1024/batch_size). if `batch_size` is 16, you can put\n",
    "            # `max_queue_size` to 64.\n",
    "            max_queue_size=64,\n",
    "            # WARNING : It is obligatory to set `workers` to 1.\n",
    "            # This in principle controls the number of workers used\n",
    "            # by keras to load mini-batches from disk to memory in parallel\n",
    "            # to GPU training. But I don't like the way it works and their\n",
    "            # code is not very commented/used, so I dont trust it that much\n",
    "            # (we might have surprises).\n",
    "            # The way it works in keras is by launching in parallel `workers`\n",
    "            # threads or processes which will all use a copy of the generator\n",
    "            # passed to `fit_generator`. So if nothing is done and `workers`\n",
    "            # is set to some number > 1, the neural net will be trained with\n",
    "            # repetitions of the same data, because the workers are independent\n",
    "            # and they got through the same generator.\n",
    "            # Hence it is necessary to introduce a shared lock between the the\n",
    "            # processes so that they load different data, this can become a bit\n",
    "            # complicated, so I choose to rather load exactly one chunk at a\n",
    "            # time using 1 worker (so `workers` have to be equal to 1), but\n",
    "            # do this single chunk loading in parallel with joblib.\n",
    "            workers=1,\n",
    "            use_multiprocessing=True,\n",
    "            validation_data=gen_valid,\n",
    "            validation_steps=get_nb_minibatches(nb_valid, batch_size),\n",
    "            verbose=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def _build_model(self):\n",
    "        inp = Input((1, 28, 28))\n",
    "        x = Flatten(name='flatten')(inp)\n",
    "        x = Dense(100, activation='relu', name='fc1')(x)\n",
    "        out = Dense(10, activation='softmax', name='predictions')(x)\n",
    "        model = Model(inp, out)\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=SGD(lr=1e-4),\n",
    "            metrics=['accuracy'])\n",
    "        return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local testing (before submission)\n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. For this we provide a unit test. Note that the test runs on your files in [`submissions/starting_kit`](/tree/submissions/starting_kit), not on the classes defined in the cells of this notebook.\n",
    "\n",
    "First `pip install ramp-workflow` or install it from the [github repo](https://github.com/paris-saclay-cds/ramp-workflow). Make sure that the python files `image_preprocessor.py` and `batch_classifier.py` are in the  [`submissions/starting_kit`](/tree/submissions/starting_kit) folder, and the data `train.csv` and `test.csv` are in [`data`](/tree/data). If you haven't yet, downlad the images by executing `python download_data.py`. Then run\n",
    "\n",
    "```ramp_test_submission```\n",
    "\n",
    "If it runs and print training and test errors on each fold, then you can submit the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MNIST classification\n",
      "Reading train and test files from ./data ...\n",
      "Reading cv ...\n",
      "Training ./submissions/starting_kit ...\n",
      "Using TensorFlow backend.\n",
      "Epoch 1/1\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "1080/1080 [==============================] - 36s - loss: 9.0336 - acc: 0.4248 - val_loss: 7.1904 - val_acc: 0.5393\n",
      "Warning: model can't be pickled.\n",
      "can't set attributes of built-in/extension type 'tuple'\n",
      "CV fold 0\n",
      "\ttrain acc = 0.55\n",
      "\tvalid acc = 0.55\n",
      "\ttest acc = 0.55\n",
      "\ttrain nll = 14.53\n",
      "\tvalid nll = 14.65\n",
      "\ttest nll = 14.6\n",
      "----------------------------\n",
      "train acc = 0.55 ± 0.0\n",
      "train nll = 14.53 ± 0.0\n",
      "valid acc = 0.55 ± 0.0\n",
      "valid nll = 14.65 ± 0.0\n",
      "test acc = 0.55 ± 0.0\n",
      "test nll = 14.6 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "!ramp_test_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to [ramp.studio](http://ramp.studio)\n",
    "\n",
    "Once you found a good feature extractor and classifier, you can submit them to [ramp.studio](http://www.ramp.studio). First, if it is your first time using RAMP, [sign up](http://www.ramp.studio/sign_up), otherwise [log in](http://www.ramp.studio/login). Then find an open event on the particular problem, for example, the event [MNIST](http://www.ramp.studio/events/MNIST) for this RAMP. Sign up for the event. Both signups are controled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request is accepted, you can go to your [sandbox](http://www.ramp.studio/events/MNIST/sandbox) and copy-paste (or upload) [`image_preprocessor.py`](/edit/submissions/starting_kit/image_preprocessor.py) and [`batch_classifier.py`](/edit/submissions/starting_kit/batch_classifier.py) from `submissions/starting_kit`. Save it, rename it, then submit it. The submission is trained and tested on our backend in the same way as `ramp_test_submission` does it locally. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in [my submissions](http://www.ramp.studio/events/MNIST/my_submissions). Once it is trained, you get a mail, and your submission shows up on the [public leaderboard](http://www.ramp.studio/events/MNIST/leaderboard). \n",
    "If there is an error (despite having tested your submission locally with `ramp_test_submission`), it will show up in the \"Failed submissions\" table in [my submissions](http://www.ramp.studio/events/MNIST/my_submissions). You can click on the error to see part of the trace.\n",
    "\n",
    "After submission, do not forget to give credits to the previous submissions you reused or integrated into your submission.\n",
    "\n",
    "The data set we use at the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The usual way to work with RAMP is to explore solutions, add feature transformations, select models, perhaps do some AutoML/hyperopt, etc., _locally_, and checking them with `ramp_test_submission`. The script prints mean cross-validation scores \n",
    "```\n",
    "----------------------------\n",
    "train acc = 0.55 ± 0.0\n",
    "train nll = 14.53 ± 0.0\n",
    "valid acc = 0.55 ± 0.0\n",
    "valid nll = 14.65 ± 0.0\n",
    "test acc = 0.55 ± 0.0\n",
    "test nll = 14.6 ± 0.0\n",
    "```\n",
    "The official score in this RAMP (the first score column after \"historical contributivity\" on the [leaderboard](http://www.ramp.studio/events/MNIST/leaderboard)) is balanced accuracy aka macro-averaged recall, so the line that is relevant in the output of `ramp_test_submission` is `valid acc = 0.55 ± 0.0`. When the score is good enough, you can submit it at the RAMP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the [README](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/README.md) of the [ramp-workflow library](https://github.com/paris-saclay-cds/ramp-workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "Don't hesitate to [contact us](mailto:admin@ramp.studio?subject=MNIST notebook)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
